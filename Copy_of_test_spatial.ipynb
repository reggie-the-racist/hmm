{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "kIszO-Fm-vSR"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "from google.colab import drive\n",
        "from google.colab import files\n",
        "from keras import callbacks\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM, Bidirectional\n",
        "import random\n",
        "from scipy.interpolate import UnivariateSpline\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: Upload Data Function\n",
        "def upload_data():\n",
        "    \"\"\"Upload your sensor data files\"\"\"\n",
        "    uploaded = files.upload()\n",
        "    return list(uploaded.keys())"
      ],
      "metadata": {
        "id": "R0A5V2cj_Ad_"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4: Data Loading and Preprocessing Functions\n",
        "def load_and_prepare_data(file_paths):\n",
        "    \"\"\"\n",
        "    Load and prepare the indoor air quality sensor data\n",
        "    \"\"\"\n",
        "    # For a single file approach (recommended for time series)\n",
        "    all_data = []\n",
        "\n",
        "    for file_path in file_paths:\n",
        "        # Load data\n",
        "        df = pd.read_csv(file_path)\n",
        "\n",
        "        # Convert datetime to datetime object\n",
        "        df['datetime'] = pd.to_datetime(df['datetime'])\n",
        "\n",
        "        # Sort by datetime\n",
        "        df = df.sort_values('datetime')\n",
        "\n",
        "        # Extract the parameters of interest\n",
        "        selected_params = ['PM2_5.ug.m3.', 'PM10.ug.m3.', 'CO2.ppm.', 'Humidity..RH.']\n",
        "\n",
        "        print(f\"Loaded data from {file_path}\")\n",
        "        print(f\"Total rows: {len(df)}\")\n",
        "\n",
        "        # Store the data in the correct format for LSTM\n",
        "        # For LSTM, we need [features, time_steps]\n",
        "        param_data = []\n",
        "        for param in selected_params:\n",
        "            param_data.append(df[param].values)\n",
        "\n",
        "        all_data.append(param_data)\n",
        "\n",
        "    # Print diagnostic information\n",
        "    for i, data_set in enumerate(all_data):\n",
        "        print(f\"Dataset {i+1} structure:\")\n",
        "        print(f\"  Number of parameters: {len(data_set)}\")\n",
        "        print(f\"  Time steps per parameter: {len(data_set[0])}\")\n",
        "\n",
        "    return all_data\n"
      ],
      "metadata": {
        "id": "MBjM0jHtHeI1"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5: Data Split Function\n",
        "def data_split(dat, train_hour, test_hour, test_period, stride):\n",
        "    \"\"\"\n",
        "    Takes as input an array of list representing a time series (array of vectors).\n",
        "\n",
        "    Args:\n",
        "        dat: array of list representing time series\n",
        "        train_hour: number of lags used for training LSTM (previous time steps to use)\n",
        "        test_hour: how many predictions we want to make simultaneously\n",
        "        test_period: How many timesteps ahead to make prediction\n",
        "        stride: How much time to stride ahead by\n",
        "\n",
        "    Returns:\n",
        "        numpy arrays x,y consisting of features/labels in format [samples, sensors, timesteps]\n",
        "    \"\"\"\n",
        "    print('Data shape = ', len(dat), 'and ', len(dat[0]))\n",
        "    # Safety check - make sure we have enough data\n",
        "    total_required_steps = train_hour + test_period + test_hour\n",
        "    if len(dat[0]) < total_required_steps:\n",
        "        print(f\"WARNING: Not enough time steps! Have {len(dat[0])}, need at least {total_required_steps}\")\n",
        "\n",
        "        # Adjust parameters if needed\n",
        "        if len(dat[0]) > 5:  # Minimum reasonable threshold\n",
        "            # Suggest better parameters\n",
        "            suggested_train = max(1, int(len(dat[0]) * 0.6))\n",
        "            suggested_test = 1\n",
        "            suggested_period = 1\n",
        "            print(f\"Suggested parameters: train_hour={suggested_train}, test_hour={suggested_test}, test_period={suggested_period}\")\n",
        "\n",
        "     # Transpose axes to [samples, time_steps, features]\n",
        "    x = np.array(x).transpose(0, 2, 1)  # New shape: (samples, time_steps, features)\n",
        "    y = np.array(y).transpose(0, 2, 1)\n",
        "\n",
        "    print(f\"X shape: {x.shape}, Y shape: {y.shape}\")\n",
        "    return x, y"
      ],
      "metadata": {
        "id": "f3ojg-SSHgkY"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 6: Data Normalization Function (Min-Max Scaling)\n",
        "def data_normalize(dat):\n",
        "    \"\"\"\n",
        "    Normalize data using Min-Max scaling\n",
        "    \"\"\"\n",
        "    new_dat = []\n",
        "    for d in dat:\n",
        "        d_array = np.array(d)\n",
        "        mask = d_array != -1\n",
        "        valid_values = d_array[mask]\n",
        "\n",
        "        if len(valid_values) > 0:\n",
        "            min_val = np.min(valid_values)\n",
        "            max_val = np.max(valid_values)\n",
        "            # Avoid division by zero\n",
        "            denominator = max_val - min_val + 1e-7\n",
        "            d_array[mask] = (d_array[mask] - min_val) / denominator\n",
        "        else:\n",
        "            d_array[mask] = 0.0  # Default if no valid data\n",
        "\n",
        "        new_dat.append(d_array.tolist())\n",
        "\n",
        "    return new_dat"
      ],
      "metadata": {
        "id": "-7fa0NrRHi91"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 7: Interpolation Function\n",
        "def interpolate(data, mask=-1):\n",
        "    \"\"\"\n",
        "    Interpolate missing values marked with mask value\n",
        "\n",
        "    Args:\n",
        "        data: List of data arrays to interpolate\n",
        "        mask: Value representing missing data\n",
        "\n",
        "    Returns:\n",
        "        Interpolated data\n",
        "    \"\"\"\n",
        "    temp = [list(dd) for dd in data]\n",
        "    d = []\n",
        "\n",
        "    for i in range(len(temp)):\n",
        "        for j in range(len(temp[i])):\n",
        "            if temp[i][j] == mask:\n",
        "                temp[i][j] = float(\"NaN\")\n",
        "\n",
        "        # Use pandas Series for interpolation\n",
        "        df = pd.Series(temp[i]).interpolate(method='linear')\n",
        "        d.append(df.tolist())\n",
        "\n",
        "    return d"
      ],
      "metadata": {
        "id": "uQm7rZF3HlKU"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 8: Split Training and Testing Data\n",
        "def split_train(int_dat, norm_dat, train_time, predict_time, predict_position, stride, start, end):\n",
        "    \"\"\"\n",
        "    Split data into training and testing sets\n",
        "    \"\"\"\n",
        "    # Ensure we're working with the first dataset if there are multiple\n",
        "    if isinstance(int_dat[0], list):\n",
        "        int_dat = int_dat[0]\n",
        "    if isinstance(norm_dat[0], list):\n",
        "        norm_dat = norm_dat[0]\n",
        "\n",
        "    length = len(int_dat[0])  # Get number of time steps\n",
        "    print(f\"Total time steps available: {length}\")\n",
        "\n",
        "    # Calculate indices for train/test split\n",
        "    s = int(length * start)\n",
        "    e = int(length * end)\n",
        "\n",
        "    # Create training and testing sets\n",
        "    train = [n[:s] for n in norm_dat]\n",
        "    test = [m[s:e] for m in int_dat]\n",
        "\n",
        "    print(f\"Training set time steps: {len(train[0])}\")\n",
        "    print(f\"Test set time steps: {len(test[0])}\")\n",
        "\n",
        "    # Check if we have enough data\n",
        "    min_required = train_time + predict_position + predict_time\n",
        "    if len(train[0]) < min_required:\n",
        "        print(f\"WARNING: Training set has insufficient time steps. Have {len(train[0])}, need {min_required}\")\n",
        "        # Adjust parameters if necessary\n",
        "        if len(train[0]) > 5:\n",
        "            new_train_time = max(1, int(len(train[0]) * 0.5))\n",
        "            new_predict_position = 1\n",
        "            new_predict_time = 1\n",
        "            print(f\"Adjusting parameters to: train_time={new_train_time}, predict_position={new_predict_position}, predict_time={new_predict_time}\")\n",
        "            train_time = new_train_time\n",
        "            predict_position = new_predict_position\n",
        "            predict_time = new_predict_time\n",
        "\n",
        "    # Create samples\n",
        "    train_x, train_y = data_split(train, train_time, predict_time, predict_position, stride)\n",
        "    test_x, test_y = data_split(test, train_time, predict_time, predict_position, stride)\n",
        "\n",
        "    return train_x, train_y, test_x, test_y"
      ],
      "metadata": {
        "id": "IOUvE9d4Hn-B"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 9: Flatten Function\n",
        "def flatten(p):\n",
        "    \"\"\"\n",
        "    Flatten batch time series data into a matrix\n",
        "\n",
        "    Args:\n",
        "        p: Batch time series data\n",
        "\n",
        "    Returns:\n",
        "        Flattened matrix\n",
        "    \"\"\"\n",
        "    pred_y_matrix = [[] for _ in range(len(p[0]))]\n",
        "\n",
        "    for pp in p:\n",
        "        a = pp.tolist()\n",
        "        for m in range(len(a)):\n",
        "            pred_y_matrix[m] += a[m]\n",
        "\n",
        "    pred_y_matrix = np.array(pred_y_matrix)\n",
        "    return np.array(pred_y_matrix).reshape((pred_y_matrix.shape[0], pred_y_matrix.shape[1]))"
      ],
      "metadata": {
        "id": "hBBUKaQ2Hp_s"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 10: Evaluation Metrics Function\n",
        "def calculate_mae(py, ty):\n",
        "    \"\"\"\n",
        "    Calculate Mean Absolute Error for each sensor parameter\n",
        "\n",
        "    Args:\n",
        "        py: Predicted values\n",
        "        ty: True values\n",
        "\n",
        "    Returns:\n",
        "        MAE and standard deviation for each parameter\n",
        "    \"\"\"\n",
        "    print('Predict data size: ', len(py), len(py[0]))\n",
        "    print('Exact data size: ', len(ty), len(ty[0]))\n",
        "\n",
        "    mae_list = []\n",
        "    std_list = []\n",
        "\n",
        "    for i in range(len(py)):\n",
        "        # Convert back from log scale\n",
        "        pred_values = np.exp(np.array(py[i]))\n",
        "        true_values = np.exp(np.array(ty[i]))\n",
        "\n",
        "        # Calculate absolute error\n",
        "        abs_error = np.abs(pred_values - true_values)\n",
        "\n",
        "        # Calculate mean and standard deviation\n",
        "        mae_list.append(np.mean(abs_error))\n",
        "        std_list.append(np.std(abs_error))\n",
        "\n",
        "        print(f\"Parameter {i+1} - MAE: {mae_list[-1]:.4f}, STD: {std_list[-1]:.4f}\")\n",
        "\n",
        "    return mae_list, std_list"
      ],
      "metadata": {
        "id": "ctB7C6E5Hsek"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 11: Visualize Predictions Function\n",
        "def visualize_predictions(py, ty, param_names):\n",
        "    \"\"\"\n",
        "    Visualize the predictions against the true values\n",
        "\n",
        "    Args:\n",
        "        py: Predicted values\n",
        "        ty: True values\n",
        "        param_names: Names of the parameters\n",
        "    \"\"\"\n",
        "    # Convert back from log scale\n",
        "    pred_values = np.exp(np.array(py))\n",
        "    true_values = np.exp(np.array(ty))\n",
        "\n",
        "    # Create subplots\n",
        "    fig, axes = plt.subplots(len(py), 1, figsize=(15, 4 * len(py)))\n",
        "\n",
        "    for i in range(len(py)):\n",
        "        if len(py) == 1:\n",
        "            ax = axes\n",
        "        else:\n",
        "            ax = axes[i]\n",
        "\n",
        "        # Plot true values\n",
        "        ax.plot(true_values[i], label='Actual', color='blue')\n",
        "\n",
        "        # Plot predicted values\n",
        "        ax.plot(pred_values[i], label='Predicted', color='red')\n",
        "\n",
        "        # Add labels and title\n",
        "        ax.set_title(f'{param_names[i]} Prediction')\n",
        "        ax.set_xlabel('Time Steps')\n",
        "        ax.set_ylabel(param_names[i])\n",
        "        ax.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "PIfu_dTEHu_n"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 12: Heatmap Visualization Function\n",
        "def visualize_heatmaps(test_y, pred_y):\n",
        "    \"\"\"\n",
        "    Visualize heatmaps of true values, predicted values, and their difference\n",
        "\n",
        "    Args:\n",
        "        test_y: True values\n",
        "        pred_y: Predicted values\n",
        "    \"\"\"\n",
        "    # Reshape data\n",
        "    test_y = test_y.reshape(test_y.shape[0], test_y.shape[1], test_y.shape[2])\n",
        "    pred_y = pred_y.reshape(pred_y.shape[0], pred_y.shape[1], pred_y.shape[2])\n",
        "\n",
        "    # Flatten the data\n",
        "    py = flatten(pred_y)\n",
        "    ty = flatten(test_y)\n",
        "\n",
        "    # Determine min and max values for consistent color scale\n",
        "    min_val, max_val = np.min(ty), np.max(ty)\n",
        "\n",
        "    # Create heatmaps\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    ax1 = sns.heatmap(np.array(ty).T, vmin=min_val, vmax=max_val, cmap='viridis')\n",
        "    ax1.set_title('Actual Data')\n",
        "    ax1.set(xlabel='Parameters', ylabel='Time Step')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    ax2 = sns.heatmap(np.array(py).T, vmin=min_val, vmax=max_val, cmap='viridis')\n",
        "    ax2.set_title('Predicted Data')\n",
        "    ax2.set(xlabel='Parameters', ylabel='Time Step')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    error = np.array(py).T - np.array(ty).T\n",
        "    ax3 = sns.heatmap(error, cmap='coolwarm', center=0)\n",
        "    ax3.set_title('Error Map')\n",
        "    ax3.set(xlabel='Parameters', ylabel='Time Step')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Calculate and return MAE and STD\n",
        "    mae_list, std_list = calculate_mae(py, ty)\n",
        "\n",
        "    print(f\"Overall MAE: {np.mean(mae_list):.4f}\")\n",
        "    print(f\"Overall STD: {np.mean(std_list):.4f}\")\n",
        "\n",
        "    return mae_list, std_list"
      ],
      "metadata": {
        "id": "tDCudBlJHxFV"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 13: Stabilized LSTM Model\n",
        "def stacked_LSTM(X, Y, name, epochs=50, batch_size=256):\n",
        "    time_step = X.shape[1]  # Time steps\n",
        "    input_dim = X.shape[2]  # Features\n",
        "    out = Y.shape[2]\n",
        "\n",
        "    model = Sequential()\n",
        "    # Simplified architecture with gradient clipping\n",
        "    model.add(Bidirectional(LSTM(32, return_sequences=True), input_shape=(time_step, input_dim)))\n",
        "    model.add(Dense(out))\n",
        "\n",
        "    # Adam optimizer with gradient clipping\n",
        "    from keras.optimizers import Adam\n",
        "    optimizer = Adam(clipvalue=0.5)\n",
        "    model.compile(loss='mean_absolute_error', optimizer=optimizer)\n",
        "\n",
        "    # Early stopping\n",
        "    callback = callbacks.EarlyStopping(monitor='loss', patience=3, min_delta=0.001)\n",
        "\n",
        "    hist = model.fit(X, Y, epochs=epochs, validation_split=0.2,\n",
        "                     verbose=1, batch_size=batch_size, callbacks=[callback])\n",
        "\n",
        "    return model, hist"
      ],
      "metadata": {
        "id": "WiMedWyHHzV3"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 14: Main Function to Run the SPATIAL Model\n",
        "def SP_Learner(data, data_name, param_names, train_split=0.8, test_split=1.0, train_time=6,\n",
        "               predict_time=1, predict_position=1, stride=1, normalize=True, epochs=50, batch_size=256):\n",
        "    \"\"\"\n",
        "    Main function to run the SPATIAL LSTM model\n",
        "\n",
        "    Args:\n",
        "        data: List of data arrays\n",
        "        data_name: Name for the dataset\n",
        "        param_names: Names of the parameters\n",
        "        train_split: Proportion of data to use for training\n",
        "        test_split: End proportion of data\n",
        "        train_time: Number of time steps to use for training\n",
        "        predict_time: Number of time steps to predict\n",
        "        predict_position: How many time steps ahead to predict\n",
        "        stride: Stride for creating samples\n",
        "        normalize: Whether to normalize the data\n",
        "        epochs: Number of training epochs\n",
        "        batch_size: Batch size for training\n",
        "\n",
        "    Returns:\n",
        "        Predicted values, true values, error metrics, and trained model\n",
        "    \"\"\"\n",
        "    print('######################## Starting SPATIAL LSTM Training ##################################')\n",
        "\n",
        "    # Normalize data if requested\n",
        "    if normalize:\n",
        "        print('Normalizing data using log transformation')\n",
        "        norm_dat = data_normalize(data)\n",
        "        norm_int_dat = interpolate(norm_dat, -1)\n",
        "    else:\n",
        "        print('Using raw data without normalization')\n",
        "        norm_dat = data\n",
        "        norm_int_dat = interpolate(data, -1)\n",
        "\n",
        "    # Split data\n",
        "    train_x, train_y, test_x, test_y = split_train(\n",
        "        norm_int_dat, norm_dat, train_time, predict_time, predict_position, stride, train_split, test_split\n",
        "    )\n",
        "\n",
        "    # Debug: Check for NaNs/Infs\n",
        "    print(\"NaN in train_x:\", np.isnan(train_x).any())\n",
        "    print(\"Inf in train_x:\", np.isinf(train_x).any())\n",
        "\n",
        "    # Train model\n",
        "    model, hist = stacked_LSTM(train_x, train_y, data_name, epochs, batch_size)\n",
        "\n",
        "    # Make predictions\n",
        "    pred_y = model.predict(test_x, verbose=1)\n",
        "\n",
        "    print('Dimensions of the output data =', pred_y.shape, test_y.shape)\n",
        "\n",
        "    # Visualize results\n",
        "    mae, std = visualize_heatmaps(test_y, pred_y)\n",
        "\n",
        "    # Visualize predictions for each parameter\n",
        "    py = flatten(pred_y)\n",
        "    ty = flatten(test_y)\n",
        "    visualize_predictions(py, ty, param_names)\n",
        "\n",
        "    print('Overall MAE:', np.mean(mae), 'STD:', np.mean(std))\n",
        "    print('######################## Finished SPATIAL LSTM Training ##################################')\n",
        "\n",
        "    return py, ty, mae, std, model, hist"
      ],
      "metadata": {
        "id": "maRP2OF2H5U3"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 15: Plot Training History Function\n",
        "def plot_training_history(history):\n",
        "    \"\"\"\n",
        "    Plot the training history\n",
        "\n",
        "    Args:\n",
        "        history: Training history from model.fit()\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(12, 4))\n",
        "\n",
        "    # Plot training & validation loss\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history.history['loss'])\n",
        "    plt.plot(history.history['val_loss'])\n",
        "    plt.title('Model Loss')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend(['Train', 'Validation'], loc='upper right')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "er3Rg1x9H8Cn"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 16: Main Execution (Updated Parameters)\n",
        "# Upload your data files\n",
        "file_paths = [\n",
        "    '/content/drive/MyDrive/Ửok/Colab notebooks/Sensor/02_15mins.csv',\n",
        "    '/content/drive/MyDrive/Ửok/Colab notebooks/Sensor/04_15mins.csv'\n",
        "]\n",
        "\n",
        "# Load data\n",
        "sensor_data = load_and_prepare_data(file_paths)\n",
        "\n",
        "# Run SPATIAL LSTM with safer parameters\n",
        "predictions, actuals, mae, std, model, hist = SP_Learner(\n",
        "    data=sensor_data,\n",
        "    data_name='Indoor Air Quality',\n",
        "    param_names=param_names,\n",
        "    train_split=0.8,\n",
        "    test_split=1.0,\n",
        "    train_time=6,       # Reduced from 12 to 6 for stability\n",
        "    predict_time=1,\n",
        "    predict_position=2, # Reduced from 4 to 2\n",
        "    stride=1,\n",
        "    normalize=True,\n",
        "    epochs=30,          # Reduced epochs\n",
        "    batch_size=64       # Smaller batch size\n",
        ")\n",
        "\n",
        "# Plot training history\n",
        "plot_training_history(hist)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "id": "AiPWEKUlH_6w",
        "outputId": "9fbb15e6-9ff4-4a02-e85f-f8b98160f423"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded data from /content/drive/MyDrive/Ửok/Colab notebooks/Sensor/02_15mins.csv\n",
            "Total rows: 17570\n",
            "Loaded data from /content/drive/MyDrive/Ửok/Colab notebooks/Sensor/04_15mins.csv\n",
            "Total rows: 17570\n",
            "Dataset 1 structure:\n",
            "  Number of parameters: 4\n",
            "  Time steps per parameter: 17570\n",
            "Dataset 2 structure:\n",
            "  Number of parameters: 4\n",
            "  Time steps per parameter: 17570\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'param_names' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-86c27d0a022e>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msensor_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mdata_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Indoor Air Quality'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mparam_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparam_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mtrain_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mtest_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'param_names' is not defined"
          ]
        }
      ]
    }
  ]
}